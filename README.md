# WordFreq Pipeline

## Project Summary

**What is this?**  
A modular, Airflow-orchestrated ETL pipeline that extracts a `.txt` file, transforms it by cleaning and tokenizing the text, and loads the results into PostgreSQL as a word frequency table. It enables unique insights into the text through frequency analysis and visualization.

**What for?**  
The goal of this project is to demonstrate core data engineering concepts using literary text as raw inputâ€”showcasing ETL architecture, orchestration, data cleaning, and structured output generation.

## Project structure


```bash
wordfreq-pipeline/
â”œâ”€â”€ dags/                        # Airflow DAGs
â”‚   â””â”€â”€ wordfreq_dag.py
â”œâ”€â”€ extract/                     # Code to extract raw .txt files
â”‚   â””â”€â”€ extract_gutenberg.py
â”œâ”€â”€ transform/                   # Text cleaning and tokenization
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ transform_utils.py
â”œâ”€â”€ load/                        # Load results into PostgreSQL
â”‚   â””â”€â”€ load_to_postgres.py
â”œâ”€â”€ data/                        # Local staging (optional, for testing)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ cleaned/
â”œâ”€â”€ s3_utils/                    # AWS S3 upload/download logic
â”‚   â””â”€â”€ s3_handler.py
â”œâ”€â”€ scripts/                     # CLI or helper scripts (e.g. local runs)
â”‚   â””â”€â”€ run_pipeline.py
â”œâ”€â”€ config/                      # Configs for S3, database, stopwords, etc.
â”‚   â”œâ”€â”€ airflow_connections.env
â”‚   â””â”€â”€ aws_config.yaml
â”œâ”€â”€ docker/                      # Docker & Airflow setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ airflow/
â”‚       â”œâ”€â”€ dags/               # Mounted into Airflow container
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ airflow.cfg
â”œâ”€â”€ notebooks/                   # (Optional) Jupyter notebooks for EDA or prototyping
â”‚   â””â”€â”€ analysis.ipynb
â”œâ”€â”€ tests/                       # Unit tests
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ .env                         # Secrets (excluded via .gitignore)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ Makefile                     # (Optional) Task automation (e.g. `make run`)
```
### S3 Bucket Structure

```bash
s3://your-bucket-name/
â”œâ”€â”€ raw/             # Original .txt files (e.g., from Project Gutenberg)
â”‚   â””â”€â”€ alice.txt
â”œâ”€â”€ cleaned/         # Cleaned .txt files (lowercase, no punctuation)
â”‚   â””â”€â”€ alice_clean.txt
â”œâ”€â”€ output/          # Final outputs like word frequency CSVs
â”‚   â””â”€â”€ wordfreq_alice.csv
```

## Pipeline Flow

Gutenberg â†’ S3 (Raw) â†’ Python ETL â†’ S3 (Processed) â†’ SQL DB â†’ Queries

1. **Ingestion Pipeline (Reusable)**  
   ğŸ“¥ Download `.txt` books from Project Gutenberg  
   â˜ï¸ Upload to Amazon S3 under `s3://your-bucket/raw/gutenberg/`

2. **Cleaning/Transformation Pipeline**  
   ğŸ§¹ Clean and tokenize text  
   â˜ï¸ Store cleaned versions in `s3://your-bucket/cleaned/gutenberg/`

3. **Loading/Analytics Pipeline**  
   ğŸ Convert to DataFrame, analyze  
   ğŸ›¢ï¸ Load word frequency data into PostgreSQL  
   ğŸ“Š Use SQL queries or dashboards for insightsion
   
## Tech Stack

- Python 3.10+

- Apache Airflow (pipeline orchestration)

- Docker + Docker Compose

- pandas (data manipulation)

- matplotlib (visualization)

- PostgreSQL
- 
- Amazon S3 (data lake storage)


## Questions This Pipeline Helps Answer

### 1. **Thematic Vocabulary**
- What are the most frequent meaningful words (after removing stopwords)?
- Do frequent terms reveal key themes? (e.g., *dream*, *time*, *king*, *soul*?)
- What are the most frequent **lemmas**?  
  *(e.g., run, runs, ran â†’ run)*

### 2. **Lexical Profile**
- What is the total vocabulary size (number of unique words)?
- What is the **type-token ratio** (lexical richness)?
- Which words appear **only once** (*hapax legomena*)?

### 3. **Keyword Presence**
- Are there **signature terms** with unusually high frequency?
- Are there **unexpected or rare words** given the genre or author?
