# WordFreq Pipeline

End-to-end text ETL from raw .txt books to PostgreSQL, using S3, Python, and Airflow.

---

## ***Index***

- [Project Overview](#project-overview)
- [Installation Instructions](#installation-instructions)
- [Project Structure](#project-structure)
- [Pipeline Flow](#pipeline-flow)
- [Tech Stack](#tech-stack)
- [Usage Examples](#usage-examples)

---

## Project Overview

***What is this?***  
A modular, Airflow-orchestrated ETL pipeline that ingests .txt books from Project Gutenberg, uploads them to Amazon S3 (raw zone), processes them with Python by cleaning and tokenizing the text, and stores the cleaned output in S3 (processed zone). The resulting word frequency data is then loaded into PostgreSQL for analysis using SQL queries.

***Applications***
This could be used as a tool to analyze documents, customer reviews, or legal texts for insight based on word frequency.

***My Personal Goal***  
Is to demonstrate core data engineering concepts using literary texts as raw inputâ€”showcasing ETL architecture, orchestration, data cleaning, and structured output generation.


## Installation Instructions

## Project structure

```bash
wordfreq-pipeline/
â”œâ”€â”€ dags/                        # Airflow DAGs
â”‚   â””â”€â”€ wordfreq_dag.py
â”œâ”€â”€ extract/                     # Code to extract raw .txt files
â”‚   â””â”€â”€ extract_gutenberg.py
â”œâ”€â”€ transform/                   # Text cleaning and tokenization
â”‚   â”œâ”€â”€ transform.py
â”‚   â””â”€â”€ transform_utils.py
â”œâ”€â”€ load/                        # Load results into PostgreSQL
â”‚   â””â”€â”€ load_to_postgres.py
â”œâ”€â”€ data/                        # Local staging (optional, for testing)
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ cleaned/
â”œâ”€â”€ s3_utils/                    # AWS S3 upload/download logic
â”‚   â””â”€â”€ s3_handler.py
â”œâ”€â”€ scripts/                     # CLI or helper scripts (e.g. local runs)
â”‚   â””â”€â”€ run_pipeline.py
â”œâ”€â”€ config/                      # Configs for S3, database, stopwords, etc.
â”‚   â”œâ”€â”€ airflow_connections.env
â”‚   â””â”€â”€ aws_config.yaml
â”œâ”€â”€ docker/                      # Docker & Airflow setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ airflow/
â”‚       â”œâ”€â”€ dags/               # Mounted into Airflow container
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ airflow.cfg
â”œâ”€â”€ notebooks/                   # (Optional) Jupyter notebooks for EDA or prototyping
â”‚   â””â”€â”€ analysis.ipynb
â”œâ”€â”€ tests/                       # Unit tests
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”œâ”€â”€ .env                         # Secrets (excluded via .gitignore)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ Makefile                     # (Optional) Task automation (e.g. `make run`)
```
### S3 Bucket Structure

```bash
s3://wordfreq-bucket-gutenberg/
â”œâ”€â”€ raw/             # Original .txt files (from Project Gutenberg)
â”‚   â””â”€â”€ book.txt
â”œâ”€â”€ cleaned/         # Cleaned .txt files (lowercase, no punctuation, no blank spaces)
â”‚   â””â”€â”€ book_clean.txt
â”œâ”€â”€ output/          # Final outputs like word frequency CSVs
â”‚   â””â”€â”€ wordfreq_book.csv
```

## Pipeline Flow

Project Gutenberg â†’ S3 (Raw) â†’ Python ETL â†’ S3 (Processed) â†’ SQL DB â†’ Queries

1. **Ingestion Pipeline (Reusable)**  
   ğŸ“¥ Download `.txt` books from Project Gutenberg  
   â˜ï¸ Upload to Amazon S3 under `s3://wordfreq-bucket-gutenberg /raw/gutenberg/`

2. **Cleaning/Transformation Pipeline**  
   ğŸ§¹ Clean and tokenize text  
   â˜ï¸ Store cleaned versions in `s3://wordfreq-bucket-gutenberg /cleaned/gutenberg/`

3. **Loading/Analytics Pipeline**  
   ğŸ Convert to DataFrame, analyze  
   ğŸ›¢ï¸ Load word frequency data into PostgreSQL  
   ğŸ“Š Use SQL queries or dashboards for insightsion
   
## Tech Stack

- Python 3.10+

- Apache Airflow (pipeline orchestration)

- Docker + Docker Compose

- pandas (data manipulation)

- matplotlib (visualization)

- PostgreSQL
- 
- Amazon S3 (data lake storage)

## Questions This Pipeline Helps Answer

### 1. **Thematic Vocabulary**
- What are the most frequent meaningful words (after removing stopwords)?
- Do frequent terms reveal key themes? (e.g., *dream*, *time*, *king*, *soul*?)
- What are the most frequent **lemmas**?  
  *(e.g., run, runs, ran â†’ run)*

### 2. **Lexical Profile**
- What is the total vocabulary size (number of unique words)?
- What is the **type-token ratio** (lexical richness)?
- Which words appear **only once** (*hapax legomena*)?

### 3. **Keyword Presence**
- Are there **signature terms** with unusually high frequency?
- Are there **unexpected or rare words** given the genre or author?

## Usage Examples


